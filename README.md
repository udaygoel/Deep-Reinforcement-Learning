# Deep-Reinforcement-Learning

Here you can find several projects covering various algorithms and implementations for deep reinforcement learning applications. These projects were developed as part of the [Udacity Deep Reinforcement Learning Nanodegree Program](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893). 



## Projects

You can view and launch the following projects:

- [Navigation - Value based methods](https://github.com/udaygoel/Deep-Reinforcement-Learning/tree/master/Navigation%20-%20Value%20based%20methods)

  This project trains an agent to navigate in a large, square world and collect bananas using Value based methods such as Deep Q-Learning Network (DQN), Double DQN (DDQN) and Prioritized Experience Reply. The Project runs through each step in detail. The objective of the agent is to receive an average reward of +13 over 100 consecutive episodes.

- [Continuous Control - Actor Critic Methods](https://github.com/udaygoel/Deep-Reinforcement-Learning/tree/master/Continuous%20Control%20-%20Actor%20Critic%20Methods)

  This project trains a double-jointed arm to move to target locations using Actor Critic Method. The algorithm uses  [Deep Deterministic Policy Gradients (DDPG)](https://arxiv.org/pdf/1509.02971.pdf_) to train the agent. The environment uses 20 agents in parallel. The objective of the agent is to  achieve an average score of at least +30 over 100 consecutive episodes and across all agents.

- [MultiAgents Tennis - Collaboration and Competition](https://github.com/udaygoel/Deep-Reinforcement-Learning/tree/master/MultiAgents%20Tennis%20-%20Collaboration%20and%20Competition)

  This project trains multiple agents (2) to play tennis using Multiple agent Actor Critic Method. The algorithm uses [Multi-Agent Deep Deterministic Policy Gradient (MADDPG)] (https://arxiv.org/abs/1706.02275) which is a multi-agent actor-critic algorithm for mixed cooperative-competitive environments. MADDPG is an extension of DDPG for the multi-agent setting. The objective of the agents is to achieve a maximum score (observed over the 2 agents) of at least +0.5 over 100 consecutive episodes.



